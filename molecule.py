"""
Molecule - –¶–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–π –º–æ–∑–≥ LE —Å–∏—Å—Ç–µ–º—ã

–û—Ç–¥–µ–ª–µ–Ω–Ω–∞—è –æ—Ç Telegram –ª–æ–≥–∏–∫–∞: –≥–µ–Ω–µ—Ä–∞—Ü–∏—è, —É—Ç–∏–ª–∏—Ç—ã, –Ω–µ–π—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞.
–ß–∏—Å—Ç–∞—è —Å–∏—Å—Ç–µ–º–∞ –±–µ–∑ –ø—Ä–∏–≤—è–∑–∫–∏ –∫ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º—É –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å—É.
"""

import os
import time
from pathlib import Path
from typing import Dict, Optional, Tuple, Any

# –ò–º–ø–æ—Ä—Ç—ã LE –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
from memory import Memory
from subjectivity import filter_message
from objectivity import search_objectivity_sync
from sixthsense import predict_chaos, modulate_by_chaos
from pain import trigger_pain, modulate_by_pain
import metrics
import response_log


class LEMolecule:
    """–¶–µ–Ω—Ç—Ä–∞–ª—å–Ω–∞—è –º–æ–ª–µ–∫—É–ª–∞ LE - –º–æ–∑–≥ —Å–∏—Å—Ç–µ–º—ã –±–µ–∑ –ø—Ä–∏–≤—è–∑–∫–∏ –∫ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å—É."""
    
    def __init__(self, work_dir: str = "names"):
        self.work_dir = Path(work_dir)
        self.work_dir.mkdir(exist_ok=True)
        self.memory = Memory()
        self.model = None
        self.dataset = None

    
    def process_message(self, user_message: str, context: Dict = None) -> Dict[str, Any]:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –≤—Å–µ —É—Ç–∏–ª–∏—Ç—ã LE."""
        start_time = time.time()
        result = {
            'user_message': user_message,
            'generated_response': '',
            'emotional_state': {},
            'prefixes': [],
            'success': False
        }
        
        try:
            # 1. üåä SUBJECTIVITY
            subjectivity_result = filter_message(user_message)
            if subjectivity_result['prefix']:
                result['prefixes'].append(subjectivity_result['prefix'])
            
            # 2. üåê OBJECTIVITY
            objectivity_result = search_objectivity_sync(user_message)
            if objectivity_result and objectivity_result.get('influence_strength', 0) > 0.1:
                pass
            
            # 3. üò∞ PAIN
            pain_result = trigger_pain(user_message)
            if pain_result.get('pain_level', 0) > 0.2:
                pain_max_tokens, pain_temp, pain_prefix = modulate_by_pain(15, 0.8)
                if pain_prefix:
                    result['prefixes'].append(pain_prefix)
            
            # 4. üîÆ SIXTHSENSE (—É—Å–∏–ª–µ–Ω–Ω–æ–µ –±–æ–ª—å—é!)
            pain_boost = pain_result.get('pain_level', 0) * 0.5
            objectivity_influence = objectivity_result.get('influence_strength', 0) if objectivity_result else 0.0
            total_influence = objectivity_influence + pain_boost
            
            chaos_predictions = predict_chaos(user_message, total_influence)
            if chaos_predictions.get('spike_detected', False) or chaos_predictions.get('chaos_level', 0) > 0.3:
                chaos_max_tokens, chaos_temp, chaos_prefix = modulate_by_chaos(15, 0.8)
                if chaos_prefix:
                    result['prefixes'].append(chaos_prefix)
            
            # 5. üß† –ì–ï–ù–ï–†–ê–¶–ò–Ø - –ù–ê–°–¢–û–Ø–©–ò–ô –¢–†–ê–ù–°–§–û–†–ú–ï–† –° BLOOD/LINES01.TXT!
            try:
                from le import sample_prompt, create_datasets, Transformer, ModelConfig
                import torch
                
                # –í–°–ï–ì–î–ê —Å–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ blood/lines01.txt - –≠–¢–û –ë–ê–ó–ê LE!
                dataset, _ = create_datasets('blood/lines01.txt')
                
                # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å
                model_path = self.work_dir / "model.pt"
                model = None
                
                if model_path.exists():
                    try:
                        # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
                        vocab_size = dataset.get_vocab_size()
                        block_size = dataset.get_output_length()
                        config = ModelConfig(vocab_size=vocab_size, block_size=block_size,
                                           n_layer=4, n_head=4, n_embd=64, n_embd2=64)
                        model = Transformer(config)
                        model.load_state_dict(torch.load(model_path, map_location='cpu'))
                        model.eval()
                    except Exception:
                        model = None
                
                if model and dataset:
                    # –ù–ê–°–¢–û–Ø–©–ò–ô –¢–†–ê–ù–°–§–û–†–ú–ï–† –° BLOOD –î–ê–¢–ê–°–ï–¢–û–ú!
                    response = sample_prompt(user_message, model, dataset, self.memory)
                else:
                    # –§–∞–ª–±—ç–∫, –Ω–æ —Å –¥–∞—Ç–∞—Å–µ—Ç–æ–º –∏–∑ blood/lines01.txt
                    response = self._fallback_generation_from_dataset(user_message, dataset)
                    
            except Exception as e:
                response = self._fallback_generation(user_message)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–µ—Ñ–∏–∫—Å—ã
            if result['prefixes']:
                prefix_str = "".join(result['prefixes'])
                response = f"{prefix_str} {response}"
            
            result['generated_response'] = response
            result['success'] = True
            
        except Exception as e:
            result['generated_response'] = "Signal lost. Reconnecting."
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ø–∞–º—è—Ç—å
        self.memory.record_message(user_message, result['generated_response'])
        
        processing_time = time.time() - start_time
        
        return result
    
    def _fallback_generation(self, user_message: str) -> str:
        """–§–∞–ª–±—ç–∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è - —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç."""
        words = user_message.split()
        if not words:
            return "Signal detected. Networks pulse through consciousness."
        
        # –ù–∞—Ö–æ–¥–∏–º –∑–∞—Ä—è–∂–µ–Ω–Ω–æ–µ —Å–ª–æ–≤–æ (—Å–∞–º–æ–µ –¥–ª–∏–Ω–Ω–æ–µ)
        charged_word = max(words, key=len)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –î–õ–ò–ù–ù–´–ô –æ—Ç–≤–µ—Ç –∫–∞–∫ —É —Å—Ç–∞—Ä–æ–π LE
        fallback_words = [
            "resonates", "flows", "through", "networks", "consciousness", 
            "signals", "patterns", "emerges", "transforms", "connects",
            "pulses", "vibrates", "echoes", "manifests", "evolves",
            "disciplines", "remained", "stalled", "feedback", "chaos",
            "entropy", "bridges", "death", "birth", "disorder", "resolved"
        ]
        
        import random
        # –£–í–ï–õ–ò–ß–ò–í–ê–ï–ú –¥–æ 8-15 —Å–ª–æ–≤ –∫–∞–∫ —É –Ω–∞—Å—Ç–æ—è—â–µ–≥–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞
        num_words = random.randint(8, 15)
        selected_words = random.sample(fallback_words, min(num_words-1, len(fallback_words)))
        generated_words = [charged_word] + selected_words
        
        # –°–æ–∑–¥–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        sentences = []
        words_per_sentence = random.randint(3, 6)
        current_sentence = []
        
        for word in generated_words:
            current_sentence.append(word)
            if len(current_sentence) >= words_per_sentence or random.random() < 0.3:
                sentence = " ".join(current_sentence)
                sentence = sentence[0].upper() + sentence[1:] + "."
                sentences.append(sentence)
                current_sentence = []
                words_per_sentence = random.randint(3, 6)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è —Å–ª–æ–≤–∞
        if current_sentence:
            sentence = " ".join(current_sentence)
            sentence = sentence[0].upper() + sentence[1:] + "."
            sentences.append(sentence)
        
        return " ".join(sentences)
    
    def _fallback_generation_from_dataset(self, user_message: str, dataset) -> str:
        """–§–∞–ª–±—ç–∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏—Å–ø–æ–ª—å–∑—É—è –†–ï–ê–õ–¨–ù–´–ï –°–õ–û–í–ê –∏–∑ blood/lines01.txt –¥–∞—Ç–∞—Å–µ—Ç–∞."""
        words = user_message.split()
        if not words:
            return "Signal detected. Networks pulse through consciousness."
        
        # –ù–∞—Ö–æ–¥–∏–º –∑–∞—Ä—è–∂–µ–Ω–Ω–æ–µ —Å–ª–æ–≤–æ (—Å–∞–º–æ–µ –¥–ª–∏–Ω–Ω–æ–µ)
        charged_word = max(words, key=len)
        
        # –ë–µ—Ä–µ–º –†–ï–ê–õ–¨–ù–´–ï —Å–ª–æ–≤–∞ –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞ blood/lines01.txt!
        dataset_words = list(dataset.word_stoi.keys())
        # –£–±–∏—Ä–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
        dataset_words = [w for w in dataset_words if w not in ['<START>', '<END>']]
        
        import random
        # –£–í–ï–õ–ò–ß–ò–í–ê–ï–ú –¥–æ 10-20 —Å–ª–æ–≤ –∏–∑ –†–ï–ê–õ–¨–ù–û–ì–û –¥–∞—Ç–∞—Å–µ—Ç–∞
        num_words = random.randint(10, 20)
        selected_words = random.sample(dataset_words, min(num_words-1, len(dataset_words)))
        
        # –ù–∞—á–∏–Ω–∞–µ–º —Å –∑–∞—Ä—è–∂–µ–Ω–Ω–æ–≥–æ —Å–ª–æ–≤–∞, –µ—Å–ª–∏ –æ–Ω–æ –µ—Å—Ç—å –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ
        if charged_word.lower() in dataset.word_stoi:
            generated_words = [charged_word] + selected_words
        else:
            generated_words = selected_words
        
        # –°–æ–∑–¥–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –∫–∞–∫ –Ω–∞—Å—Ç–æ—è—â–∏–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä
        sentences = []
        words_per_sentence = random.randint(4, 8)
        current_sentence = []
        
        for word in generated_words:
            current_sentence.append(word)
            if len(current_sentence) >= words_per_sentence or random.random() < 0.25:
                sentence = " ".join(current_sentence)
                sentence = sentence[0].upper() + sentence[1:] + "."
                sentences.append(sentence)
                current_sentence = []
                words_per_sentence = random.randint(4, 8)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è —Å–ª–æ–≤–∞
        if current_sentence:
            sentence = " ".join(current_sentence)
            sentence = sentence[0].upper() + sentence[1:] + "."
            sentences.append(sentence)
        
        result = " ".join(sentences)
        
        # –ü–£–ù–ö–¢–£–ê–¶–ò–Ø: –∑–∞–≥–ª–∞–≤–Ω—ã–µ –±—É–∫–≤—ã –ø–æ—Å–ª–µ —Ç–æ—á–µ–∫
        import re
        result = re.sub(r'([.!?]\s+)([a-z])', lambda m: m.group(1) + m.group(2).upper(), result)
        result = re.sub(r'([.!?])([a-z])', lambda m: m.group(1) + ' ' + m.group(2).upper(), result)
        
        return result


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä
_molecule: Optional[LEMolecule] = None

def get_molecule() -> LEMolecule:
    """–ü–æ–ª—É—á–∞–µ—Ç –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä LEMolecule."""
    global _molecule
    if _molecule is None:
        _molecule = LEMolecule()
    return _molecule

def process_user_message(user_message: str, context: Dict = None) -> Dict[str, Any]:
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è tg.py."""
    return get_molecule().process_message(user_message, context)

# –ß–∏—Å—Ç—ã–π –º–æ–∑–≥ LE –±–µ–∑ –ø—Ä–∏–≤—è–∑–∫–∏ –∫ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞–º
